{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cceab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from models.i3d.i3d_src.i3d_net import I3D\n",
    "from models.raft.extract_raft import DATASET_to_RAFT_CKPT_PATHS\n",
    "from models.raft.raft_src.raft import RAFT, InputPadder\n",
    "\n",
    "from models.transforms import (Clamp, PermuteAndUnsqueeze, PILToTensor,\n",
    "                               ResizeImproved, ScaleTo1_1, TensorCenterCrop,\n",
    "                               ToFloat, ToUInt8)\n",
    "\n",
    "from utils.utils import dp_state_to_normal\n",
    "\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ed05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, I3D_weight_path, RAFT_weight_path, img_stack_size, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.padder = None\n",
    "        self.stack_size = img_stack_size\n",
    "\n",
    "        # self.i3d_model_rgb = I3D(num_classes=400, modality='rgb')\n",
    "        # self.i3d_model_rgb.load_state_dict(torch.load(I3D_weight_path['rgb'], map_location='cpu'))\n",
    "        # self.i3d_model_rgb = self.i3d_model_rgb.to(self.device).eval()\n",
    "\n",
    "        # self.i3d_model_flow = I3D(num_classes=400, modality='flow')\n",
    "        # self.i3d_model_flow.load_state_dict(torch.load(I3D_weight_path['flow'], map_location='cpu'))\n",
    "        # self.i3d_model_flow = self.i3d_model_flow.to(self.device).eval()\n",
    "\n",
    "        self.raft_model = RAFT()\n",
    "        state_dict = dp_state_to_normal(torch.load(RAFT_weight_path, map_location='cpu'))\n",
    "        self.raft_model.load_state_dict(state_dict)\n",
    "        self.raft_model = self.raft_model.to(self.device).eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        rgb_stack = torch.cat(x).to(self.device)\n",
    "        stream_slice = self.raft_model(self.padder.pad(rgb_stack)[:-1], self.padder.pad(rgb_stack)[1:])\n",
    "\n",
    "        return stream_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5633c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 864.00 MiB (GPU 0; 7.78 GiB total capacity; 5.63 GiB already allocated; 241.69 MiB free; 5.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m rgb_stack\u001b[39m.\u001b[39mappend(rgb)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rgb_stack) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m stack_size:\n\u001b[0;32m---> 38\u001b[0m     result \u001b[39m=\u001b[39m model(rgb_stack)\n\u001b[1;32m     39\u001b[0m     \u001b[39mprint\u001b[39m(result\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     rgb_stack \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(x)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 24\u001b[0m     stream_slice \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraft_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadder\u001b[39m.\u001b[39;49mpad(rgb_stack)[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadder\u001b[39m.\u001b[39;49mpad(rgb_stack)[\u001b[39m1\u001b[39;49m:])\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m stream_slice\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/Action-Classification-based-Image-Audio/models/raft/raft_src/raft.py:132\u001b[0m, in \u001b[0;36mRAFT.forward\u001b[0;34m(self, image1, image2, iters, flow_init, upsample, test_mode)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m# run the feature network\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m autocast(enabled\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_precision):\n\u001b[0;32m--> 132\u001b[0m     fmap1, fmap2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfnet([image1, image2])\n\u001b[1;32m    134\u001b[0m fmap1 \u001b[39m=\u001b[39m fmap1\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    135\u001b[0m fmap2 \u001b[39m=\u001b[39m fmap2\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/Action-Classification-based-Image-Audio/models/raft/raft_src/extractor.py:177\u001b[0m, in \u001b[0;36mBasicEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n\u001b[1;32m    175\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(x)\n\u001b[0;32m--> 177\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    178\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    179\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/Action-Classification-based-Image-Audio/models/raft/raft_src/extractor.py:55\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(x)\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x \u001b[39m+\u001b[39;49m y)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 864.00 MiB (GPU 0; 7.78 GiB total capacity; 5.63 GiB already allocated; 241.69 MiB free; 5.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "video_path = \"./videos/normal.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "I3D_weight_path = {\"rgb\" : \"./models/i3d/checkpoints/i3d_rgb.pt\",\n",
    "                    \"flow\" : \"./models/i3d/checkpoints/i3d_flow.pt\"}\n",
    "\n",
    "RAFT_weight_path = \"./models/raft/checkpoints/raft-sintel.pth\"\n",
    "\n",
    "min_side_size = 256\n",
    "\n",
    "resize_transforms = torchvision.transforms.Compose([torchvision.transforms.ToPILImage(),\n",
    "                                                    ResizeImproved(min_side_size),\n",
    "                                                    PILToTensor(),\n",
    "                                                    ToFloat(),])\n",
    "\n",
    "rgb_stack = []\n",
    "stack_size = 60\n",
    "\n",
    "model = CustomModel(I3D_weight_path, RAFT_weight_path, img_stack_size = stack_size, rgb_shape_size=(1280,720,3))\n",
    "\n",
    "first_frame = True\n",
    "while cap.isOpened():\n",
    "    frame_exists, rgb = cap.read()\n",
    "\n",
    "    if first_frame:\n",
    "        first_frame = False\n",
    "        if frame_exists is False:\n",
    "            continue\n",
    "\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    rgb = resize_transforms(rgb)\n",
    "    rgb = rgb.unsqueeze(0)\n",
    "\n",
    "    rgb_stack.append(rgb)\n",
    "\n",
    "    if len(rgb_stack) - 1 == stack_size:\n",
    "        result = model(rgb_stack)\n",
    "        print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669df38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf075a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
